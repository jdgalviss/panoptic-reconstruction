{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f5d10f-6db7-4491-86bf-5597db95dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from lib.structures.field_list import collect\n",
    "\n",
    "from lib import utils, logger, config, modeling, solver, data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import lib.data.transforms2d as t2d\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lib.utils.intrinsics import adjust_intrinsic\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542945d-aec2-4779-b18d-8ffc0de3a77f",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b26fd31-73fb-408b-9c21-e0a690266ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_name = \"0012\"\n",
    "input_path = \"data/front3d/70f7c6c1-c48f-4106-bcef-40b80b84bbad/rgb_{}.png\".format(img_name)\n",
    "config.OUTPUT_DIR='output/00/{}'.format(img_name)\n",
    "config.merge_from_file('configs/front3d_train_3d_test.yaml')\n",
    "\n",
    "# inference settings\n",
    "config.MODEL.FRUSTUM3D.IS_LEVEL_64 = False\n",
    "config.MODEL.FRUSTUM3D.IS_LEVEL_128 = False\n",
    "config.MODEL.FRUSTUM3D.IS_LEVEL_256 = False\n",
    "config.MODEL.FRUSTUM3D.FIX = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb05b8c-f2c2-4a07-93a7-ee2ff441dd66",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e4661ca-aeaa-4375-8866-bd39e81809fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model...\n",
      "-----------------------------------------\n",
      "unet_output_channels 16\n",
      "unet_fetures 16\n",
      "#params discriminator 173808\n",
      "Number of Trainable Parameters: 28450735\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Load model...\")\n",
    "model = modeling.PanopticReconstruction()\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of Trainable Parameters: {}\".format(pytorch_total_params))\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "checkpoint = torch.load(config.MODEL.PRETRAIN)\n",
    "pretrained_dict = checkpoint[\"model\"]\n",
    "model_dict.update(pretrained_dict)\n",
    "# model.load_state_dict(checkpoint[\"model\"])  # load model checkpoint\n",
    "model.load_state_dict(model_dict) # Load pretrained parameters\n",
    "model = model.to(device)  # move to gpu\n",
    "model.switch_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36030f04-f32e-4508-b511-2f5818ea9546",
   "metadata": {},
   "source": [
    "## Load Image and perform inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b03fc32-a375-4f0e-80c0-1e1cb2d0aaf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load input image...\n",
      "Perform panoptic 3D scene reconstruction...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/projection/sparse_projection.py:195: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  padding_offsets = difference // 2\n",
      "/usr/local/lib/python3.8/dist-packages/MinkowskiEngine-0.5.1-py3.8-linux-x86_64.egg/MinkowskiEngine/MinkowskiSparseTensor.py:512: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  coords = coords // tensor_stride\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb: torch.Size([3, 256, 256, 256])\n",
      "Inference time: 0.938607931137085\n",
      "Visualize results, save them at output/00/0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:643: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // occupancy_prediction.tensor_stride[0]\n"
     ]
    }
   ],
   "source": [
    "# Define image transformation.\n",
    "color_image_size = (320, 240)\n",
    "depth_image_size = (160, 120)\n",
    "\n",
    "imagenet_stats = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "image_transforms = t2d.Compose([\n",
    "    t2d.Resize(color_image_size),\n",
    "    t2d.ToTensor(),\n",
    "    t2d.Normalize(imagenet_stats[0], imagenet_stats[1]),  # use imagenet stats to normalize image\n",
    "])\n",
    "#Open and prepare input image.\n",
    "print(\"Load input image...\")\n",
    "input_image = Image.open(input_path)\n",
    "input_image = image_transforms(input_image)\n",
    "input_image = input_image.unsqueeze(0).to(device)\n",
    "\n",
    "# Prepare intrinsic matrix.\n",
    "front3d_intrinsic = np.array(config.MODEL.PROJECTION.INTRINSIC)\n",
    "front3d_intrinsic = adjust_intrinsic(front3d_intrinsic, color_image_size, depth_image_size)\n",
    "front3d_intrinsic = torch.from_numpy(front3d_intrinsic).to(device).float()\n",
    "\n",
    "# Prepare frustum mask.\n",
    "front3d_frustum_mask = np.load(str(\"data/frustum_mask.npz\"))[\"mask\"]\n",
    "front3d_frustum_mask = torch.from_numpy(front3d_frustum_mask).bool().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "print(\"Perform panoptic 3D scene reconstruction...\")\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    results = model.inference(input_image, front3d_intrinsic, front3d_frustum_mask)\n",
    "end = time.time()\n",
    "print(\"Inference time: {}\".format(end - start))\n",
    "print(f\"Visualize results, save them at {config.OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87030c5-f097-4f4e-8fe6-be52b3d1fcb3",
   "metadata": {},
   "source": [
    "## Save Meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22755d8e-6a12-45ae-90f0-15ce8da27b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import lib.visualize as vis\n",
    "from lib.structures.frustum import compute_camera2frustum_transform\n",
    "from lib.structures import DepthMap\n",
    "\n",
    "depth_map: DepthMap = results[\"depth\"]\n",
    "\n",
    "# visualize results\n",
    "output_path = config.OUTPUT_DIR\n",
    "output_path = Path(output_path)\n",
    "output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Save Meshes\n",
    "dense_dimensions = torch.Size([1, 1] + config.MODEL.FRUSTUM3D.GRID_DIMENSIONS)\n",
    "min_coordinates = torch.IntTensor([0, 0, 0]).to(device)\n",
    "truncation = config.MODEL.FRUSTUM3D.TRUNCATION\n",
    "iso_value = config.MODEL.FRUSTUM3D.ISO_VALUE\n",
    "\n",
    "geometry = results[\"frustum\"][\"geometry\"]\n",
    "surface, _, _ = geometry.dense(dense_dimensions, min_coordinates, default_value=truncation)\n",
    "instances = results[\"panoptic\"][\"panoptic_instances\"]\n",
    "semantics = results[\"panoptic\"][\"panoptic_semantics\"]\n",
    "color = results[\"panoptic\"][\"rgb\"]\n",
    "color = color.permute(1,2,3,0)\n",
    "\n",
    "camera2frustum = compute_camera2frustum_transform(depth_map.intrinsic_matrix.cpu(), torch.tensor(results[\"input\"].size()) / 2.0,\n",
    "                                                      config.MODEL.PROJECTION.DEPTH_MIN,\n",
    "                                                      config.MODEL.PROJECTION.DEPTH_MAX,\n",
    "                                                      config.MODEL.PROJECTION.VOXEL_SIZE)\n",
    "\n",
    "# remove padding: original grid size: [256, 256, 256] -> [231, 174, 187]\n",
    "camera2frustum[:3, 3] += (torch.tensor([256, 256, 256]) - torch.tensor([231, 174, 187])) / 2\n",
    "frustum2camera = torch.inverse(camera2frustum)\n",
    "vis.write_distance_field(surface.squeeze(), None, output_path / \"mesh_geometry.ply\", transform=frustum2camera)\n",
    "vis.write_distance_field(surface.squeeze(), color.squeeze(), output_path / \"mesh_color.ply\", transform=frustum2camera, is_color=True)\n",
    "vis.write_distance_field(surface.squeeze(), instances.squeeze(), output_path / \"mesh_instances.ply\", transform=frustum2camera)\n",
    "vis.write_distance_field(surface.squeeze(), semantics.squeeze(), output_path / \"mesh_semantics.ply\", transform=frustum2camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f058d869-55a8-4431-96c2-9b2f9c044532",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
