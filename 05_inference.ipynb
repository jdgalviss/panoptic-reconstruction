{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6f5d10f-6db7-4491-86bf-5597db95dca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from lib.structures.field_list import collect\n",
    "\n",
    "from lib import utils, logger, config, modeling, solver, data\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import lib.data.transforms2d as t2d\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from lib.utils.intrinsics import adjust_intrinsic\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e8399b-e60f-431a-9271-ce8f01e82cf9",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf24ea00-8825-49ae-93c0-c5462699ae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load model...\n",
      "-----------------------------------------\n",
      "unet_output_channels 16\n",
      "unet_fetures 16\n",
      "#params discriminator 173808\n",
      "Number of Trainable Parameters: 29023809\n"
     ]
    }
   ],
   "source": [
    "config.merge_from_file('configs/front3d_train_3d_test.yaml')\n",
    "\n",
    "# inference settings\n",
    "config.MODEL.FRUSTUM3D.IS_LEVEL_64 = False\n",
    "config.MODEL.FRUSTUM3D.IS_LEVEL_128 = False\n",
    "config.MODEL.FRUSTUM3D.IS_LEVEL_256 = False\n",
    "config.MODEL.FRUSTUM3D.FIX = True\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "print(\"Load model...\")\n",
    "model = modeling.PanopticReconstruction()\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of Trainable Parameters: {}\".format(pytorch_total_params))\n",
    "\n",
    "model_dict = model.state_dict()\n",
    "checkpoint = torch.load(config.MODEL.PRETRAIN)\n",
    "pretrained_dict = checkpoint[\"model\"]\n",
    "model_dict.update(pretrained_dict)\n",
    "# model.load_state_dict(checkpoint[\"model\"])  # load model checkpoint\n",
    "model.load_state_dict(model_dict) # Load pretrained parameters\n",
    "model = model.to(device)  # move to gpu\n",
    "model.switch_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f542945d-aec2-4779-b18d-8ffc0de3a77f",
   "metadata": {},
   "source": [
    "## Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75b625d6-4ff3-425a-8296-1a2831d1ecd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "processing image:  0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/projection/sparse_projection.py:194: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  padding_offsets = difference // 2\n",
      "/usr/local/lib/python3.8/dist-packages/MinkowskiEngine-0.5.1-py3.8-linux-x86_64.egg/MinkowskiEngine/MinkowskiSparseTensor.py:512: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  coords = coords // tensor_stride\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:758: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // occupancy_prediction.tensor_stride[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference time: 1.6115314960479736\n",
      "Visualize results, save them at output/00/21/0001\n",
      "\n",
      "processing image:  0004\n",
      "Inference time: 1.018364429473877\n",
      "Visualize results, save them at output/00/21/0004\n",
      "\n",
      "processing image:  0006\n",
      "Inference time: 0.8830006122589111\n",
      "Visualize results, save them at output/00/21/0006\n",
      "\n",
      "processing image:  0009\n",
      "Inference time: 0.9789028167724609\n",
      "Visualize results, save them at output/00/21/0009\n",
      "\n",
      "processing image:  0018\n",
      "Inference time: 0.9109153747558594\n",
      "Visualize results, save them at output/00/21/0018\n",
      "\n",
      "processing image:  0027\n",
      "Inference time: 0.9248330593109131\n",
      "Visualize results, save them at output/00/21/0027\n",
      "\n",
      "processing image:  0030\n",
      "Inference time: 0.9714713096618652\n",
      "Visualize results, save them at output/00/21/0030\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import lib.visualize as vis\n",
    "from lib.structures.frustum import compute_camera2frustum_transform\n",
    "from lib.structures import DepthMap\n",
    "\n",
    "img_names = [\"0001\",\"0004\",\"0006\",\"0009\",\"0018\",\"0027\",\"0030\"]\n",
    "for img_name in img_names:\n",
    "    print(\"\\nprocessing image: \", img_name)\n",
    "    input_path = \"data/front3d/70f7c6c1-c48f-4106-bcef-40b80b84bbad/rgb_{}.png\".format(img_name)\n",
    "    config.OUTPUT_DIR='output/00/{}/{}'.format(21,img_name)\n",
    "\n",
    "    # Define image transformation.\n",
    "    color_image_size = (320, 240)\n",
    "    depth_image_size = (160, 120)\n",
    "\n",
    "    imagenet_stats = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    image_transforms = t2d.Compose([\n",
    "        t2d.Resize(color_image_size),\n",
    "        t2d.ToTensor(),\n",
    "        t2d.Normalize(imagenet_stats[0], imagenet_stats[1]),  # use imagenet stats to normalize image\n",
    "    ])\n",
    "    #Open and prepare input image.\n",
    "    input_image = Image.open(input_path)\n",
    "    input_image = image_transforms(input_image)\n",
    "    input_image = input_image.unsqueeze(0).to(device)\n",
    "\n",
    "    # Prepare intrinsic matrix.\n",
    "    front3d_intrinsic = np.array(config.MODEL.PROJECTION.INTRINSIC)\n",
    "    front3d_intrinsic = adjust_intrinsic(front3d_intrinsic, color_image_size, depth_image_size)\n",
    "    front3d_intrinsic = torch.from_numpy(front3d_intrinsic).to(device).float()\n",
    "\n",
    "    # Prepare frustum mask.\n",
    "    front3d_frustum_mask = np.load(str(\"data/frustum_mask.npz\"))[\"mask\"]\n",
    "    front3d_frustum_mask = torch.from_numpy(front3d_frustum_mask).bool().to(device).unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    start = time.time()\n",
    "    with torch.no_grad():\n",
    "        results = model.inference(input_image, front3d_intrinsic, front3d_frustum_mask)\n",
    "    end = time.time()\n",
    "    print(\"Inference time: {}\".format(end - start))\n",
    "    print(f\"Visualize results, save them at {config.OUTPUT_DIR}\")\n",
    "    \n",
    "    \n",
    "    ## Save imgs\n",
    "    depth_map: DepthMap = results[\"depth\"]\n",
    "\n",
    "    # visualize results\n",
    "    output_path = config.OUTPUT_DIR\n",
    "    output_path = Path(output_path)\n",
    "    output_path.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    # Save Meshes\n",
    "    dense_dimensions = torch.Size([1, 1] + config.MODEL.FRUSTUM3D.GRID_DIMENSIONS)\n",
    "    min_coordinates = torch.IntTensor([0, 0, 0]).to(device)\n",
    "    truncation = config.MODEL.FRUSTUM3D.TRUNCATION\n",
    "    iso_value = config.MODEL.FRUSTUM3D.ISO_VALUE\n",
    "\n",
    "    geometry = results[\"frustum\"][\"geometry\"]\n",
    "    surface, _, _ = geometry.dense(dense_dimensions, min_coordinates, default_value=truncation)\n",
    "    instances = results[\"panoptic\"][\"panoptic_instances\"]\n",
    "    semantics = results[\"panoptic\"][\"panoptic_semantics\"]\n",
    "    color = results[\"panoptic\"][\"rgb\"]\n",
    "    color = color.permute(1,2,3,0)\n",
    "\n",
    "    camera2frustum = compute_camera2frustum_transform(depth_map.intrinsic_matrix.cpu(), torch.tensor(results[\"input\"].size()) / 2.0,\n",
    "                                                          config.MODEL.PROJECTION.DEPTH_MIN,\n",
    "                                                          config.MODEL.PROJECTION.DEPTH_MAX,\n",
    "                                                          config.MODEL.PROJECTION.VOXEL_SIZE)\n",
    "\n",
    "    # remove padding: original grid size: [256, 256, 256] -> [231, 174, 187]\n",
    "    camera2frustum[:3, 3] += (torch.tensor([256, 256, 256]) - torch.tensor([231, 174, 187])) / 2\n",
    "    frustum2camera = torch.inverse(camera2frustum)\n",
    "    vis.write_distance_field(surface.squeeze(), None, output_path / \"mesh_geometry.ply\", transform=frustum2camera)\n",
    "    vis.write_distance_field(surface.squeeze(), color.squeeze(), output_path / \"mesh_color.ply\", transform=frustum2camera, is_color=True)\n",
    "    vis.write_distance_field(surface.squeeze(), instances.squeeze(), output_path / \"mesh_instances.ply\", transform=frustum2camera)\n",
    "    vis.write_distance_field(surface.squeeze(), semantics.squeeze(), output_path / \"mesh_semantics.ply\", transform=frustum2camera)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de3228a-93fa-4d98-a9a1-60cf4b344048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4949bf4-6680-4aa3-b981-2378f26bc810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
