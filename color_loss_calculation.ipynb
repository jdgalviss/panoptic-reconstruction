{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d752da65-11a1-4ddd-998d-80ae734bf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from lib.structures.field_list import collect\n",
    "\n",
    "from lib import utils, logger, config, modeling, solver, data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f7cb6-c482-407c-833b-906fc8d2bc33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a53c09-9050-4eb6-828a-060020af6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "unet_output_channels 16\n",
      "unet_fetures 16\n"
     ]
    }
   ],
   "source": [
    "config.merge_from_file('configs/front3d_train_3d.yaml')\n",
    "\n",
    "model = modeling.PanopticReconstruction()\n",
    "device = torch.device(config.MODEL.DEVICE)\n",
    "model.to(device, non_blocking=True)\n",
    "\n",
    "model.log_model_info()\n",
    "model.fix_weights()\n",
    "\n",
    "# Setup optimizer, scheduler, checkpointer\n",
    "optimizer = torch.optim.Adam(model.parameters(), config.SOLVER.BASE_LR,\n",
    "                                          betas=(config.SOLVER.BETA_1, config.SOLVER.BETA_2),\n",
    "                                          weight_decay=config.SOLVER.WEIGHT_DECAY)\n",
    "scheduler = solver.WarmupMultiStepLR(optimizer, config.SOLVER.STEPS, config.SOLVER.GAMMA,\n",
    "                                                  warmup_factor=1,\n",
    "                                                  warmup_iters=0,\n",
    "                                                  warmup_method=\"linear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a80f8f8-02d5-4d30-81ab-fb18e17a5500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable Parameters: 10367060\n",
      "Number of Trainable Parameters: 10367060\n"
     ]
    }
   ],
   "source": [
    "model_dict = model.state_dict()\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of Trainable Parameters: {}\".format(pytorch_total_params))\n",
    "\n",
    "output_path = Path('output')\n",
    "checkpointer = utils.DetectronCheckpointer(model, optimizer, scheduler, output_path)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_data = checkpointer.load()\n",
    "\n",
    "checkpoint_arguments = {}\n",
    "checkpoint_arguments[\"iteration\"] = 0\n",
    "\n",
    "if config.SOLVER.LOAD_SCHEDULER:\n",
    "    checkpoint_arguments.update(checkpoint_data)\n",
    "\n",
    "# TODO: move to checkpointer?\n",
    "if config.MODEL.PRETRAIN2D:\n",
    "    pretrain_2d = torch.load(config.MODEL.PRETRAIN2D)\n",
    "    model.load_state_dict(pretrain_2d[\"model\"])\n",
    "    \n",
    "# Dataloader\n",
    "dataloader = data.setup_dataloader(config.DATASETS.TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3978a9-f1b6-4573-960f-97f1c21fec1b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1a1fb8-1194-4ac1-bc9c-e8e6494a786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/projection/sparse_projection.py:195: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  padding_offsets = difference // 2\n",
      "/usr/local/lib/python3.8/dist-packages/MinkowskiEngine-0.5.1-py3.8-linux-x86_64.egg/MinkowskiEngine/MinkowskiSparseTensor.py:512: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  coords = coords // tensor_stride\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 10, total_loss: 8.237290382385254\n",
      "iteration: 11, total_loss: 8.946187973022461"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:301: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:328: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:358: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 20, total_loss: 118.40057373046875\n",
      "iteration: 30, total_loss: 109.09566497802734\n",
      "iteration: 36, total_loss: 97.508529663085944"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:411: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:506: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:536: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:568: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 40, total_loss: 464.70416259765625\n",
      "iteration: 50, total_loss: 280.65881347656255\n",
      "iteration: 60, total_loss: 224.05183410644532\n",
      "iteration: 70, total_loss: 212.21794128417978\n",
      "iteration: 80, total_loss: 169.01382446289062\n",
      "iteration: 90, total_loss: 163.88758850097656\n",
      "iteration: 100, total_loss: 166.9730987548828\n",
      "iteration: 110, total_loss: 161.50872802734375\n",
      "iteration: 111, total_loss: 147.83642578125"
     ]
    }
   ],
   "source": [
    "# Switch training mode\n",
    "# self.model.switch_training()\n",
    "print(len(dataloader))\n",
    "model.switch_training()\n",
    "iteration = 0\n",
    "iteration_end = time.time()\n",
    "\n",
    "\n",
    "for idx, (image_ids, targets) in enumerate(dataloader):\n",
    "    assert targets is not None, \"error during data loading\"\n",
    "    data_time = time.time() - iteration_end\n",
    "    # Get input images\n",
    "    images = collect(targets, \"color\")\n",
    "\n",
    "    # Pass through model\n",
    "    # try:\n",
    "    losses, results = model(images, targets)\n",
    "    # except Exception as e:\n",
    "    #     print(e, \"skipping\", image_ids[0])\n",
    "    #     del targets, images\n",
    "    #     continue\n",
    "    \n",
    "    # Accumulate total loss\n",
    "    total_loss: torch.Tensor = 0.0\n",
    "    log_meters = OrderedDict()\n",
    "\n",
    "    for loss_group in losses.values():\n",
    "        for loss_name, loss in loss_group.items():\n",
    "            if torch.is_tensor(loss) and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                total_loss += loss\n",
    "                log_meters[loss_name] = loss.item()\n",
    "\n",
    "    # Loss backpropagation, optimizer & scheduler step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if torch.is_tensor(total_loss):\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        log_meters[\"total\"] = total_loss.item()\n",
    "    else:\n",
    "        log_meters[\"total\"] = total_loss\n",
    "\n",
    "    # Minkowski Engine recommendation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if iteration % config.SOLVER.CHECKPOINT_PERIOD == 0:\n",
    "        checkpointer.save(f\"model_{iteration:07d}\", **checkpoint_arguments)\n",
    "    \n",
    "    last_training_stage = model.set_current_training_stage(iteration)\n",
    "    \n",
    "    # Save additional checkpoint after hierarchy level\n",
    "    if last_training_stage is not None:\n",
    "        checkpointer.save(f\"model_{last_training_stage}_{iteration:07d}\", **checkpoint_arguments)\n",
    "        logger.info(f\"Finish {last_training_stage} hierarchy level\")\n",
    "    \n",
    "    iteration += 1\n",
    "    iteration_end = time.time()\n",
    "\n",
    "    print(\"\\riteration: {}, total_loss: {}\".format(iteration, total_loss), end=\"\")\n",
    "    if iteration%10 == 0:\n",
    "        print(\"\\riteration: {}, total_loss: {}\".format(iteration, total_loss))\n",
    "        \n",
    "    # if idx>4:\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad5f81-287f-417f-a788-081582b58475",
   "metadata": {},
   "source": [
    "## Get color prediction for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668cb8fb-de72-4b03-9fdd-76756ea5a02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results.keys())\n",
    "print(results['frustum'].keys())\n",
    "geometry_sparse_prediction = results['frustum']['geometry']\n",
    "rgb_sparse_prediction = results['frustum']['rgb']\n",
    "print(\"geometry_sparse shape: \", geometry_sparse_prediction.shape)\n",
    "print(\"rgb_sparse shape: \", rgb_sparse_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043fd603-6de0-48f2-a5a7-423d0183cf96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.structures import DepthMap\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from lib.structures.frustum import compute_camera2frustum_transform\n",
    "\n",
    "\n",
    "# def adjust_intrinsic(intrinsic: np.array, intrinsic_image_dim: Tuple, image_dim: Tuple) -> np.array:\n",
    "#     if intrinsic_image_dim == image_dim:\n",
    "#         return intrinsic\n",
    "\n",
    "#     intrinsic_return = np.copy(intrinsic)\n",
    "\n",
    "#     height_after = image_dim[1]\n",
    "#     height_before = intrinsic_image_dim[1]\n",
    "\n",
    "#     width_after = image_dim[0]\n",
    "#     width_before = intrinsic_image_dim[0]\n",
    "\n",
    "#     intrinsic_return[0, 0] *= float(width_after) / float(width_before)\n",
    "#     intrinsic_return[1, 1] *= float(height_after) / float(height_before)\n",
    "\n",
    "#     # account for cropping/padding here\n",
    "#     intrinsic_return[0, 2] *= float(width_after - 1) / float(width_before - 1)\n",
    "#     intrinsic_return[1, 2] *= float(height_after - 1) / float(height_before - 1)\n",
    "\n",
    "#     return intrinsic_return\n",
    "\n",
    "dense_dimensions = torch.Size([1, 1] + config.MODEL.FRUSTUM3D.GRID_DIMENSIONS)\n",
    "min_coordinates = torch.IntTensor([0, 0, 0]).to(device)\n",
    "truncation = config.MODEL.FRUSTUM3D.TRUNCATION\n",
    "\n",
    "# Get Dense Predictions\n",
    "geometry, _, _ = geometry_sparse_prediction.dense(dense_dimensions, min_coordinates, default_value=truncation)\n",
    "rgb, _, _ = rgb_sparse_prediction.dense(dense_dimensions, min_coordinates)\n",
    "geometry = geometry.squeeze()\n",
    "rgb = rgb.squeeze()\n",
    "print(\"input shape: \", images.shape)\n",
    "print(\"rgb: {}\".format(rgb.shape))\n",
    "print(\"rgb values: [{},{}]\".format(torch.max(rgb), torch.min(rgb)))\n",
    "print(\"geometry: {}\".format(geometry.shape))\n",
    "print(\"geometry values: [{},{}]\".format(torch.max(geometry), torch.min(geometry)))\n",
    "\n",
    "\n",
    "# # Generate Mesh and Render\n",
    "# # Prepare intrinsic matrix.\n",
    "# color_image_size = (320, 240)\n",
    "# depth_image_size = (160, 120)\n",
    "# front3d_intrinsic = np.array(config.MODEL.PROJECTION.INTRINSIC)\n",
    "# front3d_intrinsic = adjust_intrinsic(front3d_intrinsic, color_image_size, depth_image_size)\n",
    "# front3d_intrinsic = torch.from_numpy(front3d_intrinsic).to(device).float()\n",
    "\n",
    "# print('\\n camera_instrinsics: ', front3d_intrinsic)\n",
    "# camera2frustum = compute_camera2frustum_transform(front3d_intrinsic.cpu(), torch.tensor(images.size()) / 2.0,\n",
    "#                                                       config.MODEL.PROJECTION.DEPTH_MIN,\n",
    "#                                                       config.MODEL.PROJECTION.DEPTH_MAX,\n",
    "#                                                       config.MODEL.PROJECTION.VOXEL_SIZE)\n",
    "\n",
    "# camera2frustum[:3, 3] += (torch.tensor([256, 256, 256]) - torch.tensor([231, 174, 187])) / 2\n",
    "# frustum2camera = torch.inverse(camera2frustum)\n",
    "# print(\"frustum2camera: \", frustum2camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca735c-79e1-41ce-b0c2-4d3356e47081",
   "metadata": {},
   "source": [
    "## Use marching cubes to generate mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b26b7d-074f-4867-861d-6fac3f11f32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import marching_cubes as mc\n",
    "\n",
    "distance_field = geometry.clone()\n",
    "colors = rgb.clone().permute(1,2,3,0)\n",
    "\n",
    "if isinstance(distance_field, torch.Tensor):\n",
    "    distance_field = distance_field.detach().cpu().numpy()\n",
    "if isinstance(colors, torch.Tensor):\n",
    "    colors = colors.detach().cpu().numpy()\n",
    "    \n",
    "vertices_i, triangles_i = mc.marching_cubes_color(distance_field, colors, 1.0, truncation)\n",
    "colors_i = vertices_i[..., 3:]\n",
    "vertices_i = vertices_i[..., :3]\n",
    "\n",
    "vertices = torch.from_numpy(vertices_i.astype(np.float32))\n",
    "triangles = torch.from_numpy(triangles_i.astype(np.int64))\n",
    "colors_rgb = torch.from_numpy(colors_i.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "print(\"vertices shape: {}\".format(vertices.shape))\n",
    "print(\"colors shape: {}\".format(colors_rgb.shape))\n",
    "print(\"triangles shape: {}\".format(triangles.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b0bab9-4ddd-42d2-9a5e-f5088f0e23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# import torch\n",
    "# from torchmcubes import marching_cubes, grid_interp\n",
    "\n",
    "# # Grid data\n",
    "# N = 128\n",
    "# x, y, z = np.mgrid[:N, :N, :N]\n",
    "# x = (x / N).astype('float32')\n",
    "# y = (y / N).astype('float32')\n",
    "# z = (z / N).astype('float32')\n",
    "\n",
    "# # Implicit function (metaball)\n",
    "# f0 = (x - 0.35) ** 2 + (y - 0.35) ** 2 + (z - 0.35) ** 2\n",
    "# f1 = (x - 0.65) ** 2 + (y - 0.65) ** 2 + (z - 0.65) ** 2\n",
    "# u = 1.0 / f0 + 1.0 / f1\n",
    "# rgb = np.stack((x, y, z), axis=-1)\n",
    "# rgb = np.transpose(rgb, axes=(3, 2, 1, 0)).copy()\n",
    "# print(rgb.shape)\n",
    "# print(u.shape)\n",
    "# # Test\n",
    "# u = torch.from_numpy(u).cuda()\n",
    "# rgb = torch.from_numpy(rgb).cuda()\n",
    "# verts, faces = marching_cubes(u, 15.0)\n",
    "# colros = grid_interp(rgb, verts)\n",
    "\n",
    "# verts = verts.cpu().numpy()\n",
    "# faces = faces.cpu().numpy()\n",
    "# colors = colors.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85fa85e-375e-4991-93c1-70328781724c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pytorch3d.ops import cubify\n",
    "# meshes = cubify(geometry.unsqueeze(0), thresh=1.0)\n",
    "# print(type(meshes[0]))\n",
    "from torchmcubes import marching_cubes, grid_interp\n",
    "verts, faces = marching_cubes(geometry, 1.0)\n",
    "colors = grid_interp(rgb, verts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0582b129-57d0-44f8-b274-bb54845a811e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = torch.randn_like(verts)\n",
    "colors = colors-torch.min(colors)\n",
    "colors = colors/torch.max(colors)\n",
    "print(\"verts shape: \", verts.shape)\n",
    "print(\"faces shape: \", faces.shape)\n",
    "print(\"colors shape: \", colors.shape)\n",
    "print(\"colors range: [{}, {}] \".format(torch.max(colors), torch.min(colors)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479a10b-7e27-44a7-8357-bedee42c91a2",
   "metadata": {},
   "source": [
    "## Render Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be6a69-b4ea-45f3-af99-d5da49cb8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pytorch3d\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj, load_ply\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import Textures\n",
    "\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    look_at_rotation,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    HardPhongShader,\n",
    "    SoftPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    \n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Generate texture\n",
    "# tex = Textures(verts_rgb=colors_rgb)\n",
    "# mesh = Meshes(verts=[vertices], faces=[triangles], textures=tex).to(device)\n",
    "\n",
    "tex = Textures(verts_rgb=colors.unsqueeze(0))\n",
    "mesh = Meshes(verts=[verts], faces=[faces], textures=tex).to(device)\n",
    "\n",
    "# We scale normalize and center the target mesh to fit in a sphere of radius 1 \n",
    "# centered at (0,0,0). (scale, center) will be used to bring the predicted mesh \n",
    "# to its original center and scale.  Note that normalizing the target mesh, \n",
    "# speeds up the optimization but is not necessary!\n",
    "verts = mesh.verts_packed()\n",
    "N = verts.shape[0]\n",
    "center = verts.mean(0)\n",
    "scale = max((verts - center).abs().max(0)[0])\n",
    "mesh.offset_verts_(-center)\n",
    "mesh.scale_verts_((-1.0 / float(scale)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3c59f-c309-4835-9749-6a54981fd36f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple view rendering\n",
    "from plot_image_grid import image_grid\n",
    "# the number of different viewpoints from which we want to render the mesh.\n",
    "num_views = 20\n",
    "\n",
    "# Get a batch of viewing angles. \n",
    "elev = torch.linspace(130, 200, num_views)\n",
    "azim = torch.linspace(0, 360, num_views)\n",
    "\n",
    "# Place a point light in front of the object. As mentioned above, the front of \n",
    "# the cow is facing the -z direction. \n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "# Initialize an OpenGL perspective camera that represents a batch of different \n",
    "# viewing angles. All the cameras helper methods support mixed type inputs and \n",
    "# broadcasting. So we can view the camera from the a distance of dist=2.7, and \n",
    "# then specify elevation and azimuth angles for each viewpoint as tensors. \n",
    "R, T = look_at_view_transform(dist=1.5, elev=elev, azim=azim)\n",
    "R0 = look_at_rotation(T, at=((0, 0, 3.0), ), up=((0, -1, 0), ))\n",
    "\n",
    "cameras = OpenGLPerspectiveCameras(device=device, R=R, T=T)\n",
    "\n",
    "\n",
    "# We arbitrarily choose one particular view that will be used to visualize \n",
    "# results\n",
    "camera = OpenGLPerspectiveCameras(device=device, R=R[None, 1, ...], \n",
    "                                  T=T[None, 1, ...]) \n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output \n",
    "# image to be of size 128X128. As we are rendering images for visualization \n",
    "# purposes only we will set faces_per_pixel=1 and blur_radius=0.0. Refer to \n",
    "# rasterize_meshes.py for explanations of these parameters.  We also leave \n",
    "# bin_size and max_faces_per_bin to their default values of None, which sets \n",
    "# their values using heuristics and ensures that the faster coarse-to-fine \n",
    "# rasterization method is used.  Refer to docs/notes/renderer.md for an \n",
    "# explanation of the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=128, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured \n",
    "# Phong shader will interpolate the texture uv coordinates for each vertex, \n",
    "# sample from a texture image and apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=camera, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(\n",
    "        device=device, \n",
    "        cameras=camera,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a batch of meshes by repeating the cow mesh and associated textures. \n",
    "# Meshes has a useful `extend` method which allows us do this very easily. \n",
    "# This also extends the textures. \n",
    "meshes = mesh.extend(num_views)\n",
    "\n",
    "# Render the cow mesh from each viewing angle\n",
    "target_images = renderer(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "# Our multi-view cow dataset will be represented by these 2 lists of tensors,\n",
    "# each of length num_views.\n",
    "target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n",
    "target_cameras = [OpenGLPerspectiveCameras(device=device, R=R[None, i, ...], \n",
    "                                           T=T[None, i, ...]) for i in range(num_views)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb6db0f-4fec-48c0-9fea-d38b11c86d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_grid(target_images.cpu().numpy(), rows=4, cols=5, rgb=True, show_axes=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce95b30-205e-4a12-9353-c90a2fc32f5c",
   "metadata": {},
   "source": [
    "# Deprecated stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732a7c4b-c492-474c-b0cd-f4b86ecba241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lib.data import samplers, datasets, collate\n",
    "# from lib.utils.imports import import_file\n",
    "# from torch.utils import data\n",
    "\n",
    "# def build_dataset(dataset_name) -> data.Dataset:\n",
    "#     paths_catalog = import_file(\"lib.config.paths_catalog\", config.PATHS_CATALOG, True)\n",
    "#     dataset_catalog = paths_catalog.DatasetCatalog\n",
    "#     print(\"dataset_catalog: \", dataset_catalog.get(dataset_name))\n",
    "#     info = dataset_catalog.get(dataset_name)\n",
    "#     factory = getattr(datasets, info.pop(\"factory\"))\n",
    "#     info[\"fields\"] = config.DATASETS.FIELDS\n",
    "\n",
    "#     # make dataset from factory\n",
    "#     dataset = factory(**info)\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "# dataset = build_dataset(config.DATASETS.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d7ded-f133-41d1-a819-44089dab93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset))\n",
    "# print(dataset[0][1].get_field(\"color\").shape)\n",
    "# print(dataset[0][1].fields())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
