{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d752da65-11a1-4ddd-998d-80ae734bf1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import OrderedDict\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from lib.structures.field_list import collect\n",
    "\n",
    "from lib import utils, logger, config, modeling, solver, data\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301f7cb6-c482-407c-833b-906fc8d2bc33",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6a53c09-9050-4eb6-828a-060020af6f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "unet_output_channels 16\n",
      "unet_fetures 16\n"
     ]
    }
   ],
   "source": [
    "config.merge_from_file('configs/front3d_train_3d.yaml')\n",
    "\n",
    "model = modeling.PanopticReconstruction()\n",
    "device = torch.device(config.MODEL.DEVICE)\n",
    "model.to(device, non_blocking=True)\n",
    "\n",
    "model.log_model_info()\n",
    "model.fix_weights()\n",
    "\n",
    "# Setup optimizer, scheduler, checkpointer\n",
    "optimizer = torch.optim.Adam(model.parameters(), config.SOLVER.BASE_LR,\n",
    "                                          betas=(config.SOLVER.BETA_1, config.SOLVER.BETA_2),\n",
    "                                          weight_decay=config.SOLVER.WEIGHT_DECAY)\n",
    "scheduler = solver.WarmupMultiStepLR(optimizer, config.SOLVER.STEPS, config.SOLVER.GAMMA,\n",
    "                                                  warmup_factor=1,\n",
    "                                                  warmup_iters=0,\n",
    "                                                  warmup_method=\"linear\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a80f8f8-02d5-4d30-81ab-fb18e17a5500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Trainable Parameters: 10367060\n",
      "Number of Trainable Parameters: 10367060\n"
     ]
    }
   ],
   "source": [
    "model_dict = model.state_dict()\n",
    "pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(\"Number of Trainable Parameters: {}\".format(pytorch_total_params))\n",
    "\n",
    "output_path = Path('output')\n",
    "checkpointer = utils.DetectronCheckpointer(model, optimizer, scheduler, output_path)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint_data = checkpointer.load()\n",
    "\n",
    "checkpoint_arguments = {}\n",
    "checkpoint_arguments[\"iteration\"] = 0\n",
    "\n",
    "if config.SOLVER.LOAD_SCHEDULER:\n",
    "    checkpoint_arguments.update(checkpoint_data)\n",
    "\n",
    "# TODO: move to checkpointer?\n",
    "if config.MODEL.PRETRAIN2D:\n",
    "    pretrain_2d = torch.load(config.MODEL.PRETRAIN2D)\n",
    "    model.load_state_dict(pretrain_2d[\"model\"])\n",
    "    \n",
    "# Dataloader\n",
    "dataloader = data.setup_dataloader(config.DATASETS.TRAIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3978a9-f1b6-4573-960f-97f1c21fec1b",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d1a1fb8-1194-4ac1-bc9c-e8e6494a786d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/projection/sparse_projection.py:195: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  padding_offsets = difference // 2\n",
      "/usr/local/lib/python3.8/dist-packages/MinkowskiEngine-0.5.1-py3.8-linux-x86_64.egg/MinkowskiEngine/MinkowskiSparseTensor.py:512: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  coords = coords // tensor_stride\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 1, total_loss: 44.40666198730469\n",
      "iteration: 2, total_loss: 36.77486801147461\n",
      "iteration: 3, total_loss: 27.177188873291016\n",
      "iteration: 4, total_loss: 26.205913543701172\n",
      "iteration: 5, total_loss: 22.00369644165039\n",
      "iteration: 6, total_loss: 23.459089279174805\n",
      "iteration: 7, total_loss: 22.0056209564209\n",
      "iteration: 8, total_loss: 21.678905487060547\n",
      "iteration: 9, total_loss: 22.523256301879883\n",
      "iteration: 10, total_loss: 16.366127014160156\n",
      "iteration: 11, total_loss: 20.29509925842285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:301: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:328: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:358: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration: 12, total_loss: 670.5672607421875\n",
      "iteration: 13, total_loss: 637.3270263671875\n",
      "iteration: 14, total_loss: 452.6528015136719\n",
      "iteration: 15, total_loss: 457.02789306640625\n",
      "iteration: 16, total_loss: 433.9931945800781\n",
      "iteration: 17, total_loss: 354.592041015625\n",
      "iteration: 18, total_loss: 386.04913330078125\n",
      "iteration: 19, total_loss: 321.5614013671875\n",
      "iteration: 20, total_loss: 362.21820068359375\n",
      "iteration: 21, total_loss: 317.8792419433594\n",
      "iteration: 22, total_loss: 286.913818359375\n",
      "iteration: 23, total_loss: 283.99871826171875\n",
      "iteration: 24, total_loss: 256.66461181640625\n",
      "iteration: 25, total_loss: 296.6834716796875\n",
      "iteration: 26, total_loss: 281.94451904296875\n",
      "iteration: 27, total_loss: 318.0136413574219\n",
      "iteration: 28, total_loss: 236.25521850585938\n",
      "iteration: 29, total_loss: 272.92486572265625\n",
      "iteration: 30, total_loss: 290.7519836425781\n",
      "iteration: 31, total_loss: 247.19427490234375\n",
      "iteration: 32, total_loss: 239.02548217773438\n",
      "iteration: 33, total_loss: 253.05760192871094\n",
      "iteration: 34, total_loss: 196.6700439453125\n",
      "iteration: 35, total_loss: 222.22879028320312\n",
      "iteration: 36, total_loss: 258.4322204589844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:411: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:506: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:536: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n",
      "/usr/src/app/panoptic-reconstruction/lib/modeling/frustum/frustum_completion.py:568: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  predicted_coordinates[:, 1:] = predicted_coordinates[:, 1:] // prediction.tensor_stride[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rgb_prediction shape:  torch.Size([145944, 3])\n",
      "iteration: 37, total_loss: 1227.0322265625\n",
      "rgb_prediction shape:  torch.Size([148547, 3])\n",
      "iteration: 38, total_loss: 1022.6743774414062\n",
      "rgb_prediction shape:  torch.Size([153230, 3])\n",
      "iteration: 39, total_loss: 838.9295654296875\n",
      "rgb_prediction shape:  torch.Size([154216, 3])\n",
      "iteration: 40, total_loss: 768.3778076171875\n",
      "rgb_prediction shape:  torch.Size([158648, 3])\n",
      "iteration: 41, total_loss: 683.8291015625\n",
      "rgb_prediction shape:  torch.Size([163058, 3])\n",
      "iteration: 42, total_loss: 603.7972412109375\n",
      "rgb_prediction shape:  torch.Size([171828, 3])\n",
      "iteration: 43, total_loss: 612.3236694335938\n",
      "rgb_prediction shape:  torch.Size([178096, 3])\n",
      "iteration: 44, total_loss: 577.5733032226562\n",
      "rgb_prediction shape:  torch.Size([171353, 3])\n",
      "iteration: 45, total_loss: 587.7078857421875\n",
      "rgb_prediction shape:  torch.Size([171117, 3])\n",
      "iteration: 46, total_loss: 550.9248046875\n",
      "rgb_prediction shape:  torch.Size([169707, 3])\n",
      "iteration: 47, total_loss: 585.8572387695312\n",
      "rgb_prediction shape:  torch.Size([166530, 3])\n",
      "iteration: 48, total_loss: 520.2554321289062\n",
      "rgb_prediction shape:  torch.Size([165793, 3])\n",
      "iteration: 49, total_loss: 549.93408203125\n",
      "rgb_prediction shape:  torch.Size([162130, 3])\n",
      "iteration: 50, total_loss: 575.6657104492188\n",
      "rgb_prediction shape:  torch.Size([161594, 3])\n",
      "iteration: 51, total_loss: 558.9486083984375\n",
      "rgb_prediction shape:  torch.Size([160114, 3])\n",
      "iteration: 52, total_loss: 552.6251831054688\n",
      "rgb_prediction shape:  torch.Size([159117, 3])\n",
      "iteration: 53, total_loss: 487.15301513671875\n",
      "rgb_prediction shape:  torch.Size([152642, 3])\n",
      "iteration: 54, total_loss: 508.56982421875\n",
      "rgb_prediction shape:  torch.Size([153414, 3])\n",
      "iteration: 55, total_loss: 418.26251220703125\n",
      "rgb_prediction shape:  torch.Size([151665, 3])\n",
      "iteration: 56, total_loss: 489.88323974609375\n",
      "rgb_prediction shape:  torch.Size([150316, 3])\n",
      "iteration: 57, total_loss: 453.0648498535156\n",
      "rgb_prediction shape:  torch.Size([152520, 3])\n",
      "iteration: 58, total_loss: 460.5386962890625\n",
      "rgb_prediction shape:  torch.Size([150844, 3])\n",
      "iteration: 59, total_loss: 441.18951416015625\n",
      "rgb_prediction shape:  torch.Size([147413, 3])\n",
      "iteration: 60, total_loss: 371.9866638183594\n",
      "rgb_prediction shape:  torch.Size([148372, 3])\n",
      "iteration: 61, total_loss: 482.00970458984375\n",
      "rgb_prediction shape:  torch.Size([145579, 3])\n",
      "iteration: 62, total_loss: 408.4314270019531\n",
      "rgb_prediction shape:  torch.Size([144064, 3])\n",
      "iteration: 63, total_loss: 408.8360900878906\n",
      "rgb_prediction shape:  torch.Size([143914, 3])\n",
      "iteration: 64, total_loss: 401.8947448730469\n",
      "rgb_prediction shape:  torch.Size([142030, 3])\n",
      "iteration: 65, total_loss: 381.6801452636719\n",
      "rgb_prediction shape:  torch.Size([139437, 3])\n",
      "iteration: 66, total_loss: 413.792724609375\n",
      "rgb_prediction shape:  torch.Size([138198, 3])\n",
      "iteration: 67, total_loss: 350.3268737792969\n",
      "rgb_prediction shape:  torch.Size([136798, 3])\n",
      "iteration: 68, total_loss: 411.13983154296875\n",
      "rgb_prediction shape:  torch.Size([134052, 3])\n",
      "iteration: 69, total_loss: 431.1107177734375\n",
      "rgb_prediction shape:  torch.Size([132667, 3])\n",
      "iteration: 70, total_loss: 339.78436279296875\n",
      "rgb_prediction shape:  torch.Size([130844, 3])\n",
      "iteration: 71, total_loss: 415.5550537109375\n",
      "rgb_prediction shape:  torch.Size([130843, 3])\n",
      "iteration: 72, total_loss: 451.3483581542969\n",
      "rgb_prediction shape:  torch.Size([130235, 3])\n",
      "iteration: 73, total_loss: 398.97979736328125\n",
      "rgb_prediction shape:  torch.Size([129078, 3])\n",
      "iteration: 74, total_loss: 369.984619140625\n",
      "rgb_prediction shape:  torch.Size([126447, 3])\n",
      "iteration: 75, total_loss: 425.64129638671875\n",
      "rgb_prediction shape:  torch.Size([126895, 3])\n",
      "iteration: 76, total_loss: 382.38873291015625\n",
      "rgb_prediction shape:  torch.Size([124642, 3])\n",
      "iteration: 77, total_loss: 385.8711853027344\n",
      "rgb_prediction shape:  torch.Size([125774, 3])\n",
      "iteration: 78, total_loss: 357.5736389160156\n",
      "rgb_prediction shape:  torch.Size([125570, 3])\n",
      "iteration: 79, total_loss: 352.8314514160156\n",
      "rgb_prediction shape:  torch.Size([125508, 3])\n",
      "iteration: 80, total_loss: 406.2331237792969\n",
      "rgb_prediction shape:  torch.Size([124413, 3])\n",
      "iteration: 81, total_loss: 391.47314453125\n",
      "rgb_prediction shape:  torch.Size([123134, 3])\n",
      "iteration: 82, total_loss: 380.4626770019531\n",
      "rgb_prediction shape:  torch.Size([118601, 3])\n",
      "iteration: 83, total_loss: 410.46588134765625\n",
      "rgb_prediction shape:  torch.Size([122383, 3])\n",
      "iteration: 84, total_loss: 392.16510009765625\n",
      "rgb_prediction shape:  torch.Size([124887, 3])\n",
      "iteration: 85, total_loss: 417.7371826171875\n",
      "rgb_prediction shape:  torch.Size([121689, 3])\n",
      "iteration: 86, total_loss: 339.3137512207031\n",
      "rgb_prediction shape:  torch.Size([120278, 3])\n",
      "iteration: 87, total_loss: 378.287841796875\n",
      "rgb_prediction shape:  torch.Size([121647, 3])\n",
      "iteration: 88, total_loss: 357.8850402832031\n",
      "rgb_prediction shape:  torch.Size([119730, 3])\n",
      "iteration: 89, total_loss: 367.9726867675781\n",
      "rgb_prediction shape:  torch.Size([119553, 3])\n",
      "iteration: 90, total_loss: 342.7528076171875\n",
      "rgb_prediction shape:  torch.Size([117087, 3])\n",
      "iteration: 91, total_loss: 358.27911376953125\n",
      "rgb_prediction shape:  torch.Size([118332, 3])\n",
      "iteration: 92, total_loss: 328.48760986328125\n",
      "rgb_prediction shape:  torch.Size([119331, 3])\n",
      "iteration: 93, total_loss: 342.4017639160156\n",
      "rgb_prediction shape:  torch.Size([120345, 3])\n",
      "iteration: 94, total_loss: 323.5951843261719\n",
      "rgb_prediction shape:  torch.Size([117782, 3])\n",
      "iteration: 95, total_loss: 354.1689758300781\n",
      "rgb_prediction shape:  torch.Size([119540, 3])\n",
      "iteration: 96, total_loss: 342.07025146484375\n",
      "rgb_prediction shape:  torch.Size([119921, 3])\n",
      "iteration: 97, total_loss: 405.8645935058594\n",
      "rgb_prediction shape:  torch.Size([120150, 3])\n",
      "iteration: 98, total_loss: 324.6351013183594\n",
      "rgb_prediction shape:  torch.Size([122312, 3])\n",
      "iteration: 99, total_loss: 328.4369812011719\n",
      "rgb_prediction shape:  torch.Size([119574, 3])\n",
      "iteration: 100, total_loss: 376.6190185546875\n",
      "rgb_prediction shape:  torch.Size([118533, 3])\n",
      "iteration: 101, total_loss: 435.3902282714844\n",
      "rgb_prediction shape:  torch.Size([120984, 3])\n",
      "iteration: 102, total_loss: 345.89495849609375\n",
      "rgb_prediction shape:  torch.Size([119776, 3])\n",
      "iteration: 103, total_loss: 377.7227783203125\n",
      "rgb_prediction shape:  torch.Size([119059, 3])\n",
      "iteration: 104, total_loss: 467.3208312988281\n",
      "rgb_prediction shape:  torch.Size([117340, 3])\n",
      "iteration: 105, total_loss: 395.6182556152344\n",
      "rgb_prediction shape:  torch.Size([121046, 3])\n",
      "iteration: 106, total_loss: 259.953369140625\n",
      "rgb_prediction shape:  torch.Size([120134, 3])\n",
      "iteration: 107, total_loss: 329.22882080078125\n",
      "rgb_prediction shape:  torch.Size([120313, 3])\n",
      "iteration: 108, total_loss: 436.2427978515625\n",
      "rgb_prediction shape:  torch.Size([118004, 3])\n",
      "iteration: 109, total_loss: 317.9783935546875\n",
      "rgb_prediction shape:  torch.Size([120092, 3])\n",
      "iteration: 110, total_loss: 316.3544006347656\n",
      "rgb_prediction shape:  torch.Size([119575, 3])\n",
      "iteration: 111, total_loss: 303.6253967285156\n",
      "rgb_prediction shape:  torch.Size([117878, 3])\n",
      "iteration: 112, total_loss: 430.5008850097656\n",
      "rgb_prediction shape:  torch.Size([120252, 3])\n",
      "iteration: 113, total_loss: 390.10882568359375\n",
      "rgb_prediction shape:  torch.Size([119653, 3])\n",
      "iteration: 114, total_loss: 378.0924987792969\n",
      "rgb_prediction shape:  torch.Size([118471, 3])\n",
      "iteration: 115, total_loss: 395.35479736328125\n",
      "rgb_prediction shape:  torch.Size([121038, 3])\n",
      "iteration: 116, total_loss: 297.1236572265625\n",
      "rgb_prediction shape:  torch.Size([117620, 3])\n",
      "iteration: 117, total_loss: 421.88079833984375\n",
      "rgb_prediction shape:  torch.Size([119729, 3])\n",
      "iteration: 118, total_loss: 238.97799682617188\n",
      "rgb_prediction shape:  torch.Size([117699, 3])\n",
      "iteration: 119, total_loss: 416.03680419921875\n",
      "rgb_prediction shape:  torch.Size([119053, 3])\n",
      "iteration: 120, total_loss: 340.99407958984375\n",
      "rgb_prediction shape:  torch.Size([118632, 3])\n",
      "iteration: 121, total_loss: 264.6146240234375\n",
      "rgb_prediction shape:  torch.Size([119197, 3])\n",
      "iteration: 122, total_loss: 319.38079833984375\n",
      "rgb_prediction shape:  torch.Size([119295, 3])\n",
      "iteration: 123, total_loss: 353.8359375\n",
      "rgb_prediction shape:  torch.Size([118880, 3])\n",
      "iteration: 124, total_loss: 334.1348571777344\n",
      "rgb_prediction shape:  torch.Size([118717, 3])\n",
      "iteration: 125, total_loss: 279.6553649902344\n",
      "rgb_prediction shape:  torch.Size([118630, 3])\n",
      "iteration: 126, total_loss: 365.716552734375\n",
      "rgb_prediction shape:  torch.Size([117192, 3])\n",
      "iteration: 127, total_loss: 367.6292419433594\n",
      "rgb_prediction shape:  torch.Size([118810, 3])\n",
      "iteration: 128, total_loss: 342.6571350097656\n",
      "rgb_prediction shape:  torch.Size([118136, 3])\n",
      "iteration: 129, total_loss: 353.73193359375\n",
      "rgb_prediction shape:  torch.Size([118115, 3])\n",
      "iteration: 130, total_loss: 335.42138671875\n",
      "rgb_prediction shape:  torch.Size([117504, 3])\n",
      "iteration: 131, total_loss: 348.2051696777344\n",
      "rgb_prediction shape:  torch.Size([119543, 3])\n",
      "iteration: 132, total_loss: 272.4178771972656\n",
      "rgb_prediction shape:  torch.Size([119158, 3])\n",
      "iteration: 133, total_loss: 361.7758483886719\n",
      "rgb_prediction shape:  torch.Size([117452, 3])\n",
      "iteration: 134, total_loss: 284.8045959472656\n",
      "rgb_prediction shape:  torch.Size([119876, 3])\n",
      "iteration: 135, total_loss: 365.23651123046875\n",
      "rgb_prediction shape:  torch.Size([118539, 3])\n",
      "iteration: 136, total_loss: 285.5314636230469\n",
      "rgb_prediction shape:  torch.Size([118113, 3])\n",
      "iteration: 137, total_loss: 280.338134765625\n",
      "rgb_prediction shape:  torch.Size([116538, 3])\n",
      "iteration: 138, total_loss: 316.2991638183594\n",
      "rgb_prediction shape:  torch.Size([117726, 3])\n",
      "iteration: 139, total_loss: 372.044677734375\n",
      "rgb_prediction shape:  torch.Size([117911, 3])\n",
      "iteration: 140, total_loss: 434.3658752441406\n",
      "rgb_prediction shape:  torch.Size([119042, 3])\n",
      "iteration: 141, total_loss: 307.6229553222656\n",
      "rgb_prediction shape:  torch.Size([116740, 3])\n",
      "iteration: 142, total_loss: 360.0286560058594\n",
      "rgb_prediction shape:  torch.Size([118886, 3])\n",
      "iteration: 143, total_loss: 357.300048828125\n",
      "rgb_prediction shape:  torch.Size([117997, 3])\n",
      "iteration: 144, total_loss: 391.6430358886719\n",
      "rgb_prediction shape:  torch.Size([117770, 3])\n",
      "iteration: 145, total_loss: 340.15850830078125\n",
      "rgb_prediction shape:  torch.Size([119086, 3])\n",
      "iteration: 146, total_loss: 333.399658203125\n",
      "rgb_prediction shape:  torch.Size([119007, 3])\n",
      "iteration: 147, total_loss: 360.6878356933594\n",
      "rgb_prediction shape:  torch.Size([118256, 3])\n",
      "iteration: 148, total_loss: 291.78875732421875\n",
      "rgb_prediction shape:  torch.Size([118138, 3])\n",
      "iteration: 149, total_loss: 287.0681457519531\n",
      "rgb_prediction shape:  torch.Size([119293, 3])\n",
      "iteration: 150, total_loss: 401.8108215332031\n",
      "rgb_prediction shape:  torch.Size([117813, 3])\n",
      "iteration: 151, total_loss: 369.3332824707031\n",
      "rgb_prediction shape:  torch.Size([118180, 3])\n",
      "iteration: 152, total_loss: 290.0257263183594\n",
      "rgb_prediction shape:  torch.Size([120173, 3])\n",
      "iteration: 153, total_loss: 313.6741027832031\n",
      "rgb_prediction shape:  torch.Size([120266, 3])\n",
      "iteration: 154, total_loss: 253.91607666015625\n",
      "rgb_prediction shape:  torch.Size([119146, 3])\n",
      "iteration: 155, total_loss: 324.76513671875\n",
      "rgb_prediction shape:  torch.Size([118496, 3])\n",
      "iteration: 156, total_loss: 423.61865234375\n",
      "rgb_prediction shape:  torch.Size([118546, 3])\n",
      "iteration: 157, total_loss: 282.33740234375\n",
      "rgb_prediction shape:  torch.Size([118886, 3])\n",
      "iteration: 158, total_loss: 342.4305725097656\n",
      "rgb_prediction shape:  torch.Size([119128, 3])\n",
      "iteration: 159, total_loss: 339.15838623046875\n",
      "rgb_prediction shape:  torch.Size([121428, 3])\n",
      "iteration: 160, total_loss: 316.875\n",
      "rgb_prediction shape:  torch.Size([122480, 3])\n",
      "iteration: 161, total_loss: 286.9098205566406\n",
      "rgb_prediction shape:  torch.Size([119748, 3])\n",
      "iteration: 162, total_loss: 322.5415954589844\n",
      "rgb_prediction shape:  torch.Size([119541, 3])\n",
      "iteration: 163, total_loss: 339.1788024902344\n",
      "rgb_prediction shape:  torch.Size([119158, 3])\n",
      "iteration: 164, total_loss: 310.3894348144531\n",
      "rgb_prediction shape:  torch.Size([117990, 3])\n",
      "iteration: 165, total_loss: 289.4336853027344\n",
      "rgb_prediction shape:  torch.Size([121233, 3])\n",
      "iteration: 166, total_loss: 332.420654296875\n",
      "rgb_prediction shape:  torch.Size([118321, 3])\n",
      "iteration: 167, total_loss: 313.478271484375\n",
      "rgb_prediction shape:  torch.Size([117176, 3])\n",
      "iteration: 168, total_loss: 387.9010009765625\n",
      "rgb_prediction shape:  torch.Size([116945, 3])\n",
      "iteration: 169, total_loss: 381.4142761230469\n",
      "rgb_prediction shape:  torch.Size([118407, 3])\n",
      "iteration: 170, total_loss: 371.1747131347656\n",
      "rgb_prediction shape:  torch.Size([119305, 3])\n",
      "iteration: 171, total_loss: 337.8784484863281\n",
      "rgb_prediction shape:  torch.Size([118340, 3])\n",
      "iteration: 172, total_loss: 411.9538879394531\n",
      "rgb_prediction shape:  torch.Size([118119, 3])\n",
      "iteration: 173, total_loss: 368.0502014160156\n",
      "rgb_prediction shape:  torch.Size([117460, 3])\n",
      "iteration: 174, total_loss: 280.94775390625\n",
      "rgb_prediction shape:  torch.Size([118322, 3])\n",
      "iteration: 175, total_loss: 357.03466796875\n",
      "rgb_prediction shape:  torch.Size([119692, 3])\n",
      "iteration: 176, total_loss: 333.2877197265625\n",
      "rgb_prediction shape:  torch.Size([119478, 3])\n",
      "iteration: 177, total_loss: 301.84674072265625\n",
      "rgb_prediction shape:  torch.Size([119310, 3])\n",
      "iteration: 178, total_loss: 334.76690673828125\n",
      "rgb_prediction shape:  torch.Size([117608, 3])\n",
      "iteration: 179, total_loss: 377.261474609375\n",
      "rgb_prediction shape:  torch.Size([118813, 3])\n",
      "iteration: 180, total_loss: 340.29339599609375\n",
      "rgb_prediction shape:  torch.Size([117882, 3])\n",
      "iteration: 181, total_loss: 363.20623779296875\n",
      "rgb_prediction shape:  torch.Size([119378, 3])\n",
      "iteration: 182, total_loss: 277.1549987792969\n",
      "rgb_prediction shape:  torch.Size([117014, 3])\n",
      "iteration: 183, total_loss: 362.080078125\n",
      "rgb_prediction shape:  torch.Size([117003, 3])\n",
      "iteration: 184, total_loss: 281.78729248046875\n",
      "rgb_prediction shape:  torch.Size([117172, 3])\n",
      "iteration: 185, total_loss: 410.1197509765625\n",
      "rgb_prediction shape:  torch.Size([119419, 3])\n",
      "iteration: 186, total_loss: 307.3564453125\n",
      "rgb_prediction shape:  torch.Size([118903, 3])\n",
      "iteration: 187, total_loss: 356.2709655761719\n",
      "rgb_prediction shape:  torch.Size([118842, 3])\n",
      "iteration: 188, total_loss: 334.8821105957031\n",
      "rgb_prediction shape:  torch.Size([118449, 3])\n",
      "iteration: 189, total_loss: 360.9569396972656\n",
      "rgb_prediction shape:  torch.Size([119983, 3])\n",
      "iteration: 190, total_loss: 302.341796875\n",
      "rgb_prediction shape:  torch.Size([118695, 3])\n",
      "iteration: 191, total_loss: 339.9857177734375\n",
      "rgb_prediction shape:  torch.Size([119279, 3])\n",
      "iteration: 192, total_loss: 391.90985107421875\n",
      "rgb_prediction shape:  torch.Size([118377, 3])\n",
      "iteration: 193, total_loss: 363.84906005859375\n",
      "rgb_prediction shape:  torch.Size([118383, 3])\n",
      "iteration: 194, total_loss: 325.6141662597656\n",
      "rgb_prediction shape:  torch.Size([121205, 3])\n",
      "iteration: 195, total_loss: 333.4610290527344\n",
      "rgb_prediction shape:  torch.Size([120094, 3])\n",
      "iteration: 196, total_loss: 255.83419799804688\n",
      "rgb_prediction shape:  torch.Size([118092, 3])\n",
      "iteration: 197, total_loss: 296.91278076171875\n",
      "rgb_prediction shape:  torch.Size([117340, 3])\n",
      "iteration: 198, total_loss: 338.3852844238281\n",
      "rgb_prediction shape:  torch.Size([117823, 3])\n",
      "iteration: 199, total_loss: 396.87255859375\n",
      "rgb_prediction shape:  torch.Size([117890, 3])\n",
      "iteration: 200, total_loss: 431.4570617675781\n",
      "rgb_prediction shape:  torch.Size([119105, 3])\n",
      "iteration: 201, total_loss: 250.59524536132812\n",
      "rgb_prediction shape:  torch.Size([116793, 3])\n",
      "iteration: 202, total_loss: 334.0179748535156\n",
      "rgb_prediction shape:  torch.Size([118050, 3])\n",
      "iteration: 203, total_loss: 383.1705627441406\n",
      "rgb_prediction shape:  torch.Size([119700, 3])\n",
      "iteration: 204, total_loss: 333.556884765625\n",
      "rgb_prediction shape:  torch.Size([118108, 3])\n",
      "iteration: 205, total_loss: 338.7184143066406\n",
      "rgb_prediction shape:  torch.Size([117716, 3])\n",
      "iteration: 206, total_loss: 278.80987548828125\n",
      "rgb_prediction shape:  torch.Size([119366, 3])\n",
      "iteration: 207, total_loss: 319.6495056152344\n",
      "rgb_prediction shape:  torch.Size([117079, 3])\n",
      "iteration: 208, total_loss: 333.7064208984375\n",
      "rgb_prediction shape:  torch.Size([119334, 3])\n",
      "iteration: 209, total_loss: 347.8252868652344\n",
      "rgb_prediction shape:  torch.Size([119760, 3])\n",
      "iteration: 210, total_loss: 293.08843994140625\n",
      "rgb_prediction shape:  torch.Size([117434, 3])\n",
      "iteration: 211, total_loss: 272.8675842285156\n",
      "rgb_prediction shape:  torch.Size([120479, 3])\n",
      "iteration: 212, total_loss: 280.676513671875\n",
      "rgb_prediction shape:  torch.Size([119841, 3])\n",
      "iteration: 213, total_loss: 294.5509033203125\n",
      "rgb_prediction shape:  torch.Size([119576, 3])\n",
      "iteration: 214, total_loss: 294.8034362792969\n",
      "rgb_prediction shape:  torch.Size([121015, 3])\n",
      "iteration: 215, total_loss: 385.1487731933594\n",
      "rgb_prediction shape:  torch.Size([118539, 3])\n",
      "iteration: 216, total_loss: 250.24557495117188\n",
      "rgb_prediction shape:  torch.Size([120431, 3])\n",
      "iteration: 217, total_loss: 379.01129150390625\n",
      "rgb_prediction shape:  torch.Size([118724, 3])\n",
      "iteration: 218, total_loss: 345.3572998046875\n",
      "rgb_prediction shape:  torch.Size([118585, 3])\n",
      "iteration: 219, total_loss: 271.8499755859375\n",
      "rgb_prediction shape:  torch.Size([119041, 3])\n",
      "iteration: 220, total_loss: 294.8736267089844\n",
      "rgb_prediction shape:  torch.Size([120878, 3])\n",
      "iteration: 221, total_loss: 343.95037841796875\n",
      "rgb_prediction shape:  torch.Size([118879, 3])\n",
      "iteration: 222, total_loss: 290.1732482910156\n",
      "rgb_prediction shape:  torch.Size([119554, 3])\n",
      "iteration: 223, total_loss: 328.0208740234375\n",
      "rgb_prediction shape:  torch.Size([122006, 3])\n",
      "iteration: 224, total_loss: 324.6440734863281\n",
      "rgb_prediction shape:  torch.Size([119166, 3])\n",
      "iteration: 225, total_loss: 296.4495849609375\n",
      "rgb_prediction shape:  torch.Size([118148, 3])\n",
      "iteration: 226, total_loss: 340.75128173828125\n",
      "rgb_prediction shape:  torch.Size([121183, 3])\n",
      "iteration: 227, total_loss: 359.77471923828125\n",
      "rgb_prediction shape:  torch.Size([120261, 3])\n",
      "iteration: 228, total_loss: 308.42327880859375\n",
      "rgb_prediction shape:  torch.Size([117677, 3])\n",
      "iteration: 229, total_loss: 322.6790466308594\n",
      "rgb_prediction shape:  torch.Size([116525, 3])\n",
      "iteration: 230, total_loss: 355.86614990234375\n",
      "rgb_prediction shape:  torch.Size([117595, 3])\n",
      "iteration: 231, total_loss: 390.0480651855469\n",
      "rgb_prediction shape:  torch.Size([118545, 3])\n",
      "iteration: 232, total_loss: 348.79931640625\n",
      "rgb_prediction shape:  torch.Size([117614, 3])\n",
      "iteration: 233, total_loss: 330.91748046875\n",
      "rgb_prediction shape:  torch.Size([120044, 3])\n",
      "iteration: 234, total_loss: 264.53765869140625\n",
      "rgb_prediction shape:  torch.Size([123160, 3])\n",
      "iteration: 235, total_loss: 322.0661315917969\n",
      "rgb_prediction shape:  torch.Size([119933, 3])\n",
      "iteration: 236, total_loss: 259.92156982421875\n",
      "rgb_prediction shape:  torch.Size([119037, 3])\n",
      "iteration: 237, total_loss: 415.9342041015625\n",
      "rgb_prediction shape:  torch.Size([121929, 3])\n",
      "iteration: 238, total_loss: 248.04713439941406\n",
      "rgb_prediction shape:  torch.Size([118944, 3])\n",
      "iteration: 239, total_loss: 352.81439208984375\n",
      "rgb_prediction shape:  torch.Size([117936, 3])\n",
      "iteration: 240, total_loss: 355.52880859375\n",
      "rgb_prediction shape:  torch.Size([119171, 3])\n",
      "iteration: 241, total_loss: 318.86639404296875\n",
      "rgb_prediction shape:  torch.Size([119898, 3])\n",
      "iteration: 242, total_loss: 304.9693603515625\n",
      "rgb_prediction shape:  torch.Size([121812, 3])\n",
      "iteration: 243, total_loss: 227.3123779296875\n",
      "rgb_prediction shape:  torch.Size([118496, 3])\n",
      "iteration: 244, total_loss: 330.3326110839844\n",
      "rgb_prediction shape:  torch.Size([118554, 3])\n",
      "iteration: 245, total_loss: 339.0932312011719\n",
      "rgb_prediction shape:  torch.Size([117182, 3])\n",
      "iteration: 246, total_loss: 271.0994567871094\n",
      "rgb_prediction shape:  torch.Size([120211, 3])\n",
      "iteration: 247, total_loss: 290.0355224609375\n",
      "rgb_prediction shape:  torch.Size([119253, 3])\n",
      "iteration: 248, total_loss: 332.8675537109375\n",
      "rgb_prediction shape:  torch.Size([118589, 3])\n",
      "iteration: 249, total_loss: 423.77459716796875\n",
      "rgb_prediction shape:  torch.Size([121442, 3])\n",
      "iteration: 250, total_loss: 296.82855224609375\n",
      "rgb_prediction shape:  torch.Size([119193, 3])\n",
      "iteration: 251, total_loss: 388.1300964355469\n",
      "rgb_prediction shape:  torch.Size([120238, 3])\n",
      "iteration: 252, total_loss: 329.1732482910156\n",
      "rgb_prediction shape:  torch.Size([119352, 3])\n",
      "iteration: 253, total_loss: 286.58636474609375\n",
      "rgb_prediction shape:  torch.Size([118457, 3])\n",
      "iteration: 254, total_loss: 407.8162841796875\n",
      "rgb_prediction shape:  torch.Size([118259, 3])\n",
      "iteration: 255, total_loss: 320.69171142578125\n",
      "rgb_prediction shape:  torch.Size([118834, 3])\n",
      "iteration: 256, total_loss: 289.96124267578125\n",
      "rgb_prediction shape:  torch.Size([118447, 3])\n",
      "iteration: 257, total_loss: 258.4216003417969\n",
      "rgb_prediction shape:  torch.Size([117585, 3])\n",
      "iteration: 258, total_loss: 393.97222900390625\n",
      "rgb_prediction shape:  torch.Size([118661, 3])\n",
      "iteration: 259, total_loss: 344.080078125\n",
      "rgb_prediction shape:  torch.Size([120432, 3])\n",
      "iteration: 260, total_loss: 338.5660400390625\n",
      "rgb_prediction shape:  torch.Size([120530, 3])\n",
      "iteration: 261, total_loss: 265.8775634765625\n",
      "rgb_prediction shape:  torch.Size([119292, 3])\n",
      "iteration: 262, total_loss: 326.1595458984375\n",
      "rgb_prediction shape:  torch.Size([118509, 3])\n",
      "iteration: 263, total_loss: 387.8381042480469\n",
      "rgb_prediction shape:  torch.Size([121593, 3])\n",
      "iteration: 264, total_loss: 318.15606689453125\n",
      "rgb_prediction shape:  torch.Size([121568, 3])\n",
      "iteration: 265, total_loss: 286.8249206542969\n",
      "rgb_prediction shape:  torch.Size([117066, 3])\n",
      "iteration: 266, total_loss: 321.3990173339844\n",
      "rgb_prediction shape:  torch.Size([118488, 3])\n",
      "iteration: 267, total_loss: 296.5025634765625\n",
      "rgb_prediction shape:  torch.Size([119168, 3])\n",
      "iteration: 268, total_loss: 290.9739990234375\n",
      "rgb_prediction shape:  torch.Size([118362, 3])\n",
      "iteration: 269, total_loss: 238.13169860839844\n",
      "rgb_prediction shape:  torch.Size([118583, 3])\n",
      "iteration: 270, total_loss: 352.3951110839844\n",
      "rgb_prediction shape:  torch.Size([119764, 3])\n",
      "iteration: 271, total_loss: 409.7635192871094\n",
      "rgb_prediction shape:  torch.Size([118932, 3])\n",
      "iteration: 272, total_loss: 335.0643615722656\n",
      "rgb_prediction shape:  torch.Size([117903, 3])\n",
      "iteration: 273, total_loss: 326.931640625\n",
      "rgb_prediction shape:  torch.Size([117646, 3])\n",
      "iteration: 274, total_loss: 393.549560546875\n",
      "rgb_prediction shape:  torch.Size([119606, 3])\n",
      "iteration: 275, total_loss: 347.5973815917969\n",
      "rgb_prediction shape:  torch.Size([118554, 3])\n",
      "iteration: 276, total_loss: 360.8725280761719\n",
      "rgb_prediction shape:  torch.Size([118417, 3])\n",
      "iteration: 277, total_loss: 253.80551147460938\n",
      "rgb_prediction shape:  torch.Size([119192, 3])\n",
      "iteration: 278, total_loss: 286.8868713378906\n",
      "rgb_prediction shape:  torch.Size([117387, 3])\n",
      "iteration: 279, total_loss: 339.8105163574219\n",
      "rgb_prediction shape:  torch.Size([118129, 3])\n",
      "iteration: 280, total_loss: 372.2234802246094\n",
      "rgb_prediction shape:  torch.Size([118566, 3])\n",
      "iteration: 281, total_loss: 363.71160888671875\n",
      "rgb_prediction shape:  torch.Size([118799, 3])\n",
      "iteration: 282, total_loss: 336.18487548828125\n",
      "rgb_prediction shape:  torch.Size([119323, 3])\n",
      "iteration: 283, total_loss: 330.2910461425781\n",
      "rgb_prediction shape:  torch.Size([116523, 3])\n",
      "iteration: 284, total_loss: 386.04986572265625\n",
      "rgb_prediction shape:  torch.Size([118554, 3])\n",
      "iteration: 285, total_loss: 424.9621276855469\n",
      "rgb_prediction shape:  torch.Size([120469, 3])\n",
      "iteration: 286, total_loss: 263.19683837890625\n",
      "rgb_prediction shape:  torch.Size([117935, 3])\n",
      "iteration: 287, total_loss: 327.9928283691406\n",
      "rgb_prediction shape:  torch.Size([121341, 3])\n",
      "iteration: 288, total_loss: 353.7261047363281\n",
      "rgb_prediction shape:  torch.Size([119538, 3])\n",
      "iteration: 289, total_loss: 320.552001953125\n",
      "rgb_prediction shape:  torch.Size([118877, 3])\n",
      "iteration: 290, total_loss: 277.5513916015625\n",
      "rgb_prediction shape:  torch.Size([117138, 3])\n",
      "iteration: 291, total_loss: 350.0037841796875\n",
      "rgb_prediction shape:  torch.Size([117709, 3])\n",
      "iteration: 292, total_loss: 322.92938232421875\n",
      "rgb_prediction shape:  torch.Size([118295, 3])\n",
      "iteration: 293, total_loss: 424.8623962402344\n",
      "rgb_prediction shape:  torch.Size([121835, 3])\n",
      "iteration: 294, total_loss: 346.0124816894531\n",
      "rgb_prediction shape:  torch.Size([120553, 3])\n",
      "iteration: 295, total_loss: 345.5303955078125\n",
      "rgb_prediction shape:  torch.Size([116839, 3])\n",
      "iteration: 296, total_loss: 358.702392578125\n",
      "rgb_prediction shape:  torch.Size([119027, 3])\n",
      "iteration: 297, total_loss: 278.8028564453125\n",
      "rgb_prediction shape:  torch.Size([120561, 3])\n",
      "iteration: 298, total_loss: 361.7205505371094\n",
      "rgb_prediction shape:  torch.Size([118214, 3])\n",
      "iteration: 299, total_loss: 413.5893859863281\n",
      "rgb_prediction shape:  torch.Size([118378, 3])\n",
      "iteration: 300, total_loss: 420.2494201660156\n"
     ]
    }
   ],
   "source": [
    "# Switch training mode\n",
    "# self.model.switch_training()\n",
    "print(len(dataloader))\n",
    "model.switch_training()\n",
    "iteration = 0\n",
    "iteration_end = time.time()\n",
    "\n",
    "\n",
    "for idx, (image_ids, targets) in enumerate(dataloader):\n",
    "    assert targets is not None, \"error during data loading\"\n",
    "    data_time = time.time() - iteration_end\n",
    "    # Get input images\n",
    "    images = collect(targets, \"color\")\n",
    "\n",
    "    # Pass through model\n",
    "    # try:\n",
    "    losses, results = model(images, targets)\n",
    "    # except Exception as e:\n",
    "    #     print(e, \"skipping\", image_ids[0])\n",
    "    #     del targets, images\n",
    "    #     continue\n",
    "    \n",
    "    # Accumulate total loss\n",
    "    total_loss: torch.Tensor = 0.0\n",
    "    log_meters = OrderedDict()\n",
    "\n",
    "    for loss_group in losses.values():\n",
    "        for loss_name, loss in loss_group.items():\n",
    "            if torch.is_tensor(loss) and not torch.isnan(loss) and not torch.isinf(loss):\n",
    "                total_loss += loss\n",
    "                log_meters[loss_name] = loss.item()\n",
    "\n",
    "    # Loss backpropagation, optimizer & scheduler step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if torch.is_tensor(total_loss):\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        log_meters[\"total\"] = total_loss.item()\n",
    "    else:\n",
    "        log_meters[\"total\"] = total_loss\n",
    "\n",
    "    # Minkowski Engine recommendation\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    if iteration % config.SOLVER.CHECKPOINT_PERIOD == 0:\n",
    "        checkpointer.save(f\"model_{iteration:07d}\", **checkpoint_arguments)\n",
    "    \n",
    "    last_training_stage = model.set_current_training_stage(iteration)\n",
    "    \n",
    "    # Save additional checkpoint after hierarchy level\n",
    "    if last_training_stage is not None:\n",
    "        checkpointer.save(f\"model_{last_training_stage}_{iteration:07d}\", **checkpoint_arguments)\n",
    "        logger.info(f\"Finish {last_training_stage} hierarchy level\")\n",
    "    \n",
    "    iteration += 1\n",
    "    iteration_end = time.time()\n",
    "\n",
    "    print(\"\\riteration: {}, total_loss: {}\".format(iteration, total_loss), end=\"\")\n",
    "    # if idx>4:\n",
    "        # break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad5f81-287f-417f-a788-081582b58475",
   "metadata": {},
   "source": [
    "## Get color prediction for rendering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "668cb8fb-de72-4b03-9fdd-76756ea5a02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['depth', 'instance', 'projection', 'frustum'])\n",
      "dict_keys(['occupancy_64', 'instance3d_64', 'semantic3d_64', 'occupancy_128', 'instance3d_128', 'semantic3d_128', 'occupancy_256', 'geometry', 'instance3d', 'instance3d_prediction', 'semantic3d', 'semantic3d_label', 'rgb'])\n",
      "geometry_sparse shape:  torch.Size([118378, 1])\n",
      "rgb_sparse shape:  torch.Size([118378, 3])\n"
     ]
    }
   ],
   "source": [
    "print(results.keys())\n",
    "print(results['frustum'].keys())\n",
    "geometry_sparse_prediction = results['frustum']['geometry']\n",
    "rgb_sparse_prediction = results['frustum']['rgb']\n",
    "print(\"geometry_sparse shape: \", geometry_sparse_prediction.shape)\n",
    "print(\"rgb_sparse shape: \", rgb_sparse_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "043fd603-6de0-48f2-a5a7-423d0183cf96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape:  torch.Size([1, 3, 240, 320])\n",
      "rgb: torch.Size([3, 256, 256, 256])\n",
      "rgb values: [3.132716178894043,0.0]\n",
      "geometry: torch.Size([256, 256, 256])\n",
      "geometry values: [3.0,0.038527555763721466]\n",
      "\n",
      " camera_instrinsics:  tensor([[138.5641,   0.0000,  79.5000,   0.0000],\n",
      "        [  0.0000, 138.5641,  59.5000,   0.0000],\n",
      "        [  0.0000,   0.0000,   1.0000,   0.0000],\n",
      "        [  0.0000,   0.0000,   0.0000,   1.0000]], device='cuda:0')\n",
      "frustum2camera:  tensor([[ 0.0300,  0.0000,  0.0000, -3.8175],\n",
      "        [ 0.0000,  0.0300,  0.0000, -3.8064],\n",
      "        [ 0.0000,  0.0000,  0.0300, -0.6350],\n",
      "        [ 0.0000,  0.0000,  0.0000,  1.0000]])\n"
     ]
    }
   ],
   "source": [
    "from lib.structures import DepthMap\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "from lib.structures.frustum import compute_camera2frustum_transform\n",
    "\n",
    "\n",
    "def adjust_intrinsic(intrinsic: np.array, intrinsic_image_dim: Tuple, image_dim: Tuple) -> np.array:\n",
    "    if intrinsic_image_dim == image_dim:\n",
    "        return intrinsic\n",
    "\n",
    "    intrinsic_return = np.copy(intrinsic)\n",
    "\n",
    "    height_after = image_dim[1]\n",
    "    height_before = intrinsic_image_dim[1]\n",
    "\n",
    "    width_after = image_dim[0]\n",
    "    width_before = intrinsic_image_dim[0]\n",
    "\n",
    "    intrinsic_return[0, 0] *= float(width_after) / float(width_before)\n",
    "    intrinsic_return[1, 1] *= float(height_after) / float(height_before)\n",
    "\n",
    "    # account for cropping/padding here\n",
    "    intrinsic_return[0, 2] *= float(width_after - 1) / float(width_before - 1)\n",
    "    intrinsic_return[1, 2] *= float(height_after - 1) / float(height_before - 1)\n",
    "\n",
    "    return intrinsic_return\n",
    "\n",
    "dense_dimensions = torch.Size([1, 1] + config.MODEL.FRUSTUM3D.GRID_DIMENSIONS)\n",
    "min_coordinates = torch.IntTensor([0, 0, 0]).to(device)\n",
    "truncation = config.MODEL.FRUSTUM3D.TRUNCATION\n",
    "\n",
    "# Get Dense Predictions\n",
    "geometry, _, _ = geometry_sparse_prediction.dense(dense_dimensions, min_coordinates, default_value=truncation)\n",
    "rgb, _, _ = rgb_sparse_prediction.dense(dense_dimensions, min_coordinates)\n",
    "geometry = geometry.squeeze()\n",
    "rgb = rgb.squeeze()\n",
    "print(\"input shape: \", images.shape)\n",
    "print(\"rgb: {}\".format(rgb.shape))\n",
    "print(\"rgb values: [{},{}]\".format(torch.max(rgb), torch.min(rgb)))\n",
    "print(\"geometry: {}\".format(geometry.shape))\n",
    "print(\"geometry values: [{},{}]\".format(torch.max(geometry), torch.min(geometry)))\n",
    "\n",
    "\n",
    "# Generate Mesh and Render\n",
    "# Prepare intrinsic matrix.\n",
    "color_image_size = (320, 240)\n",
    "depth_image_size = (160, 120)\n",
    "front3d_intrinsic = np.array(config.MODEL.PROJECTION.INTRINSIC)\n",
    "front3d_intrinsic = adjust_intrinsic(front3d_intrinsic, color_image_size, depth_image_size)\n",
    "front3d_intrinsic = torch.from_numpy(front3d_intrinsic).to(device).float()\n",
    "\n",
    "print('\\n camera_instrinsics: ', front3d_intrinsic)\n",
    "camera2frustum = compute_camera2frustum_transform(front3d_intrinsic.cpu(), torch.tensor(images.size()) / 2.0,\n",
    "                                                      config.MODEL.PROJECTION.DEPTH_MIN,\n",
    "                                                      config.MODEL.PROJECTION.DEPTH_MAX,\n",
    "                                                      config.MODEL.PROJECTION.VOXEL_SIZE)\n",
    "\n",
    "camera2frustum[:3, 3] += (torch.tensor([256, 256, 256]) - torch.tensor([231, 174, 187])) / 2\n",
    "frustum2camera = torch.inverse(camera2frustum)\n",
    "print(\"frustum2camera: \", frustum2camera)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca735c-79e1-41ce-b0c2-4d3356e47081",
   "metadata": {},
   "source": [
    "## Use marching cubes to generate mesh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29b26b7d-074f-4867-861d-6fac3f11f32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vertices shape: torch.Size([39034, 3])\n",
      "colors shape: torch.Size([1, 39034, 3])\n",
      "triangles shape: torch.Size([78092, 3])\n"
     ]
    }
   ],
   "source": [
    "import marching_cubes as mc\n",
    "\n",
    "distance_field = geometry.clone()\n",
    "colors = rgb.clone().permute(1,2,3,0)\n",
    "\n",
    "if isinstance(distance_field, torch.Tensor):\n",
    "    distance_field = distance_field.detach().cpu().numpy()\n",
    "if isinstance(colors, torch.Tensor):\n",
    "    colors = colors.detach().cpu().numpy()\n",
    "    \n",
    "vertices_i, triangles_i = mc.marching_cubes_color(distance_field, colors, 1.0, truncation)\n",
    "colors_i = vertices_i[..., 3:]\n",
    "vertices_i = vertices_i[..., :3]\n",
    "\n",
    "vertices = torch.from_numpy(vertices_i.astype(np.float32))\n",
    "triangles = torch.from_numpy(triangles_i.astype(np.int64))\n",
    "colors_rgb = torch.from_numpy(colors_i.astype(np.float32)).unsqueeze(0)\n",
    "\n",
    "print(\"vertices shape: {}\".format(vertices.shape))\n",
    "print(\"colors shape: {}\".format(colors_rgb.shape))\n",
    "print(\"triangles shape: {}\".format(triangles.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b85fa85e-375e-4991-93c1-70328781724c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "si\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.cuda' has no attribute 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [28]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n\u001b[0;32m---> 11\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.cuda' has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "from pytorch3d.ops import cubify\n",
    "# meshes = cubify(geometry.unsqueeze(0), thresh=1.0)\n",
    "# print(type(meshes))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2479a10b-7e27-44a7-8357-bedee42c91a2",
   "metadata": {},
   "source": [
    "## Render Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07be6a69-b4ea-45f3-af99-d5da49cb8250",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import pytorch3d\n",
    "# Util function for loading meshes\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj, load_ply\n",
    "\n",
    "# Data structures and functions for rendering\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import Textures\n",
    "\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    look_at_rotation,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    HardPhongShader,\n",
    "    SoftPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    \n",
    ")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.cuda.set_device(device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "# Generate texture\n",
    "tex = Textures(verts_rgb=colors_rgb)\n",
    "mesh = Meshes(verts=[vertices], faces=[triangles], textures=tex).to(device)\n",
    "\n",
    "# We scale normalize and center the target mesh to fit in a sphere of radius 1 \n",
    "# centered at (0,0,0). (scale, center) will be used to bring the predicted mesh \n",
    "# to its original center and scale.  Note that normalizing the target mesh, \n",
    "# speeds up the optimization but is not necessary!\n",
    "verts = mesh.verts_packed()\n",
    "N = verts.shape[0]\n",
    "center = verts.mean(0)\n",
    "scale = max((verts - center).abs().max(0)[0])\n",
    "mesh.offset_verts_(-center)\n",
    "mesh.scale_verts_((1.0 / float(scale)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ba3c59f-c309-4835-9749-6a54981fd36f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Meshes does not have textures",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 64>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m meshes \u001b[38;5;241m=\u001b[39m mesh\u001b[38;5;241m.\u001b[39mextend(num_views)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# Render the cow mesh from each viewing angle\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m target_images \u001b[38;5;241m=\u001b[39m \u001b[43mrenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmeshes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcameras\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcameras\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Our multi-view cow dataset will be represented by these 2 lists of tensors,\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# each of length num_views.\u001b[39;00m\n\u001b[1;32m     68\u001b[0m target_rgb \u001b[38;5;241m=\u001b[39m [target_images[i, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_views)]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch3d/renderer/mesh/renderer.py:64\u001b[0m, in \u001b[0;36mMeshRenderer.forward\u001b[0;34m(self, meshes_world, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mRender a batch of images from a batch of meshes by rasterizing and then\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03mshading.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;124;03mFor this set rasterizer.raster_settings.clip_barycentric_coords=True\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     63\u001b[0m fragments \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrasterizer(meshes_world, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 64\u001b[0m images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfragments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeshes_world\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m images\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch3d/renderer/mesh/shader.py:84\u001b[0m, in \u001b[0;36mHardPhongShader.forward\u001b[0;34m(self, fragments, meshes, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCameras must be specified either at initialization \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;124m        or in the forward pass of HardPhongShader\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m---> 84\u001b[0m texels \u001b[38;5;241m=\u001b[39m \u001b[43mmeshes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_textures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfragments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m lights \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlights\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlights)\n\u001b[1;32m     86\u001b[0m materials \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaterials\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaterials)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pytorch3d/structures/meshes.py:1557\u001b[0m, in \u001b[0;36mMeshes.sample_textures\u001b[0;34m(self, fragments)\u001b[0m\n\u001b[1;32m   1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtextures\u001b[38;5;241m.\u001b[39msample_textures(\n\u001b[1;32m   1554\u001b[0m         fragments, faces_packed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfaces_packed()\n\u001b[1;32m   1555\u001b[0m     )\n\u001b[1;32m   1556\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1557\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMeshes does not have textures\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Meshes does not have textures"
     ]
    }
   ],
   "source": [
    "# Multiple view rendering\n",
    "from plot_image_grid import image_grid\n",
    "# the number of different viewpoints from which we want to render the mesh.\n",
    "num_views = 20\n",
    "\n",
    "# Get a batch of viewing angles. \n",
    "elev = torch.linspace(140, 190, num_views)\n",
    "azim = torch.linspace(-40, 40, num_views)\n",
    "\n",
    "# Place a point light in front of the object. As mentioned above, the front of \n",
    "# the cow is facing the -z direction. \n",
    "lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n",
    "\n",
    "# Initialize an OpenGL perspective camera that represents a batch of different \n",
    "# viewing angles. All the cameras helper methods support mixed type inputs and \n",
    "# broadcasting. So we can view the camera from the a distance of dist=2.7, and \n",
    "# then specify elevation and azimuth angles for each viewpoint as tensors. \n",
    "R, T = look_at_view_transform(dist=1.0, elev=elev, azim=azim)\n",
    "R0 = look_at_rotation(T, at=((0, 0, 3.0), ), up=((0, -1, 0), ))\n",
    "\n",
    "cameras = OpenGLPerspectiveCameras(device=device, R=R0@R, T=T)\n",
    "\n",
    "\n",
    "# We arbitrarily choose one particular view that will be used to visualize \n",
    "# results\n",
    "camera = OpenGLPerspectiveCameras(device=device, R=R[None, 1, ...], \n",
    "                                  T=T[None, 1, ...]) \n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output \n",
    "# image to be of size 128X128. As we are rendering images for visualization \n",
    "# purposes only we will set faces_per_pixel=1 and blur_radius=0.0. Refer to \n",
    "# rasterize_meshes.py for explanations of these parameters.  We also leave \n",
    "# bin_size and max_faces_per_bin to their default values of None, which sets \n",
    "# their values using heuristics and ensures that the faster coarse-to-fine \n",
    "# rasterization method is used.  Refer to docs/notes/renderer.md for an \n",
    "# explanation of the difference between naive and coarse-to-fine rasterization. \n",
    "raster_settings = RasterizationSettings(\n",
    "    image_size=128, \n",
    "    blur_radius=0.0, \n",
    "    faces_per_pixel=1, \n",
    ")\n",
    "\n",
    "# Create a Phong renderer by composing a rasterizer and a shader. The textured \n",
    "# Phong shader will interpolate the texture uv coordinates for each vertex, \n",
    "# sample from a texture image and apply the Phong lighting model\n",
    "renderer = MeshRenderer(\n",
    "    rasterizer=MeshRasterizer(\n",
    "        cameras=camera, \n",
    "        raster_settings=raster_settings\n",
    "    ),\n",
    "    shader=HardPhongShader(\n",
    "        device=device, \n",
    "        cameras=camera,\n",
    "        lights=lights\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create a batch of meshes by repeating the cow mesh and associated textures. \n",
    "# Meshes has a useful `extend` method which allows us do this very easily. \n",
    "# This also extends the textures. \n",
    "meshes = mesh.extend(num_views)\n",
    "\n",
    "# Render the cow mesh from each viewing angle\n",
    "target_images = renderer(meshes, cameras=cameras, lights=lights)\n",
    "\n",
    "# Our multi-view cow dataset will be represented by these 2 lists of tensors,\n",
    "# each of length num_views.\n",
    "target_rgb = [target_images[i, ..., :3] for i in range(num_views)]\n",
    "target_cameras = [OpenGLPerspectiveCameras(device=device, R=R[None, i, ...], \n",
    "                                           T=T[None, i, ...]) for i in range(num_views)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "acb6db0f-4fec-48c0-9fea-d38b11c86d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image_grid(target_images.cpu().numpy(), rows=4, cols=5, rgb=True, show_axes=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce95b30-205e-4a12-9353-c90a2fc32f5c",
   "metadata": {},
   "source": [
    "# Deprecated stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "732a7c4b-c492-474c-b0cd-f4b86ecba241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_catalog:  {'file_list_path': 'resources/front3d/train_list_3d_subsampled.txt', 'dataset_root_path': 'data/front3d/', 'factory': 'Front3D'}\n"
     ]
    }
   ],
   "source": [
    "# from lib.data import samplers, datasets, collate\n",
    "# from lib.utils.imports import import_file\n",
    "# from torch.utils import data\n",
    "\n",
    "# def build_dataset(dataset_name) -> data.Dataset:\n",
    "#     paths_catalog = import_file(\"lib.config.paths_catalog\", config.PATHS_CATALOG, True)\n",
    "#     dataset_catalog = paths_catalog.DatasetCatalog\n",
    "#     print(\"dataset_catalog: \", dataset_catalog.get(dataset_name))\n",
    "#     info = dataset_catalog.get(dataset_name)\n",
    "#     factory = getattr(datasets, info.pop(\"factory\"))\n",
    "#     info[\"fields\"] = config.DATASETS.FIELDS\n",
    "\n",
    "#     # make dataset from factory\n",
    "#     dataset = factory(**info)\n",
    "\n",
    "#     return dataset\n",
    "\n",
    "# dataset = build_dataset(config.DATASETS.TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d7ded-f133-41d1-a819-44089dab93a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(dataset))\n",
    "# print(dataset[0][1].get_field(\"color\").shape)\n",
    "# print(dataset[0][1].fields())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
