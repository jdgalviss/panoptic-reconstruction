{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50962483-ef96-4712-a907-1ff9e2c5c971",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch3d\n",
    "from pytorch3d.io import load_objs_as_meshes, load_obj, load_ply\n",
    "\n",
    "from pytorch3d.structures import Meshes\n",
    "from pytorch3d.renderer import Textures\n",
    "\n",
    "from pytorch3d.vis.plotly_vis import AxisArgs, plot_batch_individually, plot_scene\n",
    "from pytorch3d.vis.texture_vis import texturesuv_image_matplotlib\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    look_at_rotation,\n",
    "    FoVPerspectiveCameras, \n",
    "    PointLights, \n",
    "    DirectionalLights, \n",
    "    Materials, \n",
    "    RasterizationSettings, \n",
    "    MeshRenderer, \n",
    "    MeshRasterizer,  \n",
    "    HardPhongShader,\n",
    "    SoftPhongShader,\n",
    "    TexturesUV,\n",
    "    TexturesVertex,\n",
    "    OpenGLPerspectiveCameras, \n",
    "    PerspectiveCameras\n",
    "    \n",
    ")\n",
    "\n",
    "\n",
    "#from plot_image_grid import plot_image\n",
    "from pytorch3d.structures import Pointclouds\n",
    "from pytorch3d.renderer import (\n",
    "    look_at_view_transform,\n",
    "    FoVPerspectiveCameras, \n",
    "    PerspectiveCameras,\n",
    "    OrthographicCameras,\n",
    "    VolumeRenderer,\n",
    "    NDCGridRaysampler,\n",
    "    MonteCarloRaysampler,\n",
    "    GridRaysampler,\n",
    "    EmissionAbsorptionRaymarcher,\n",
    "    AbsorptionOnlyRaymarcher,\n",
    "    NDCMultinomialRaysampler,\n",
    "    MultinomialRaysampler,\n",
    "    PointsRasterizationSettings,\n",
    "    PointsRasterizer,\n",
    "    PointsRenderer,\n",
    "    AlphaCompositor\n",
    ")\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72fc157d-8df1-4605-8f0a-931a7fa295f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "color_image_size = (240, 320)\n",
    "FOCAL_LENGTH1 = 277.1281435\n",
    "FOCAL_LENGTH2 = 311.76912635\n",
    "\n",
    "R, T = look_at_view_transform(-3.1, 0, 0, up=((0, -1, 0), ))\n",
    "# T[0][2] += -0.2\n",
    "# T[0][0] += -0.2\n",
    "# R, T = look_at_view_transform(eye= torch.FloatTensor([0.0,0.0,-3.1]).unsqueeze(0), at=((0.0, 0, 0), ), up=((0, -1, 0), ))\n",
    "raster_settings = PointsRasterizationSettings(\n",
    "    image_size=color_image_size, \n",
    "    radius = 0.03,\n",
    "    points_per_pixel = 10\n",
    ")\n",
    "\n",
    "# cameras = FoVOrthographicCameras(device=device, R=R, T=T, znear=0.01)\n",
    "cameras = PerspectiveCameras(device=device, R=R, T=T, focal_length=((FOCAL_LENGTH1,FOCAL_LENGTH1),),\n",
    "                             principal_point=(([160,120]),), image_size=((240, 320), ),in_ndc=False) #, K=front3d_intrinsic.unsqueeze(0))\\\n",
    "\n",
    "# Define the settings for rasterization and shading. Here we set the output image to be of size\n",
    "# 512x512. As we are rendering images for visualization purposes only we will set faces_per_pixel=1\n",
    "# and blur_radius=0.0. Refer to raster_points.py for explanations of these parameters. \n",
    "\n",
    "\n",
    "\n",
    "# Create a points renderer by compositing points using an alpha compositor (nearer points\n",
    "# are weighted more heavily). See [1] for an explanation.\n",
    "rasterizer = PointsRasterizer(cameras=cameras, raster_settings=raster_settings)\n",
    "renderer = PointsRenderer(\n",
    "    rasterizer=rasterizer,\n",
    "    compositor=AlphaCompositor()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e66c049-60ec-459e-9fca-a5a17455789f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'point_cloud' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m images \u001b[38;5;241m=\u001b[39m renderer(\u001b[43mpoint_cloud\u001b[49m)\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m images[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m plot_image(image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'point_cloud' is not defined"
     ]
    }
   ],
   "source": [
    "verts, faces = load_ply(\"pc.ply\")\n",
    "\n",
    "point_cloud = Pointclouds(points=[points.type(torch.FloatTensor)], features=[points_rgb.type(torch.FloatTensor)]).to(device)\n",
    "\n",
    "images = renderer(point_cloud)\n",
    "image = images[0, ..., :3].cpu().detach().numpy()\n",
    "plot_image(image)\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "input_img = Image.open('data/front3d/70f7c6c1-c48f-4106-bcef-40b80b84bbad/rgb_0001.png')\n",
    "plot_image(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500c04e7-8ec4-42ac-9314-78d901c34110",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
